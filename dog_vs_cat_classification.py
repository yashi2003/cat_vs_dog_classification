# -*- coding: utf-8 -*-
"""dog vs cat classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rBOHjT2axzIVCDu7r-2pW6h0CKrPvMEI
"""

!mkdir  -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

#now copy the api command
!kaggle datasets download -d salader/dogs-vs-cats

"""### Our dataset is in .zip format. now we will unzip it"""

import zipfile
zip_ref = zipfile.ZipFile("/content/dogs-vs-cats.zip")
zip_ref.extractall("/content")
zip_ref.close()

"""# Now we will load the libraries"""

import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten

"""## Now we will use the concept of generators to load the data in batches and at the same time we customize it as well ie label encoding cat = 0 dog =1"""

train_ds = keras.utils.image_dataset_from_directory(
    directory = '/content/train',
    labels = 'inferred',
    #cat will be 0 and dog will be 1 but they can be in string

    label_mode = 'int', #for converting them in string
    batch_size = 32,
    image_size = (256,256)
)

validation_ds = keras.utils.image_dataset_from_directory(
    directory = '/content/test',
    labels = 'inferred',
    #cat will be 0 and dog will be 1 but they can be in string

    label_mode = 'int', #for converting them in string
    batch_size = 32,
    image_size = (256,256)
)

"""### Normalizing the dataset"""

def process(image, label):
  image = tf.cast(image/255, tf.float32)
  return image, label

train_ds = train_ds.map(process)
validation_ds = validation_ds.map(process)

"""## Creating a CNN model"""

model1 = Sequential()

model1.add(Conv2D(32, kernel_size= (3,3), padding = 'valid', activation= 'relu', input_shape= (256,256,3)))
#padding = if any location misses out it will get filled with zero

model1.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid')),

model1.add(Conv2D(64, kernel_size= (3,3), padding= 'valid', activation='relu'))
model1.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid')),

model1.add(Conv2D(128, kernel_size = (3,3), padding= 'valid', activation='relu'))
model1.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid')),

model1.add(Flatten())

model1.add(Dense(128, activation = 'relu'))
model1.add(Dense(64, activation = 'relu'))
model1.add(Dense(1, activation = 'sigmoid'))

model1.summary()

model1.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics= ['accuracy'])

history = model1.fit(train_ds, epochs= 10, validation_data = validation_ds)

"""we passed validation_data because we wanted to loss and accuracy for test and training data both in

training acc = 99% and testing acc = 80%
## RESULT = OVERFITTING
"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], color = 'red', label='train')
plt.plot(history.history['val_accuracy'], color = 'blue', label = 'validation')
plt.legend()
plt.show()

"""### wrt loss"""

plt.plot(history.history['loss'], color = 'red', label='train')
plt.plot(history.history['val_loss'], color = 'blue', label = 'validation')
plt.legend()
plt.show()

"""## Batch Regularization ie regularize our model and dropout"""

from keras.layers import BatchNormalization, Dropout

model1 = Sequential()

model1.add(Conv2D(32, kernel_size= (3,3), padding = 'valid', activation= 'relu', input_shape= (256,256,3)))
#padding = if any location misses out it will get filled with zero
model1.add(BatchNormalization())
model1.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid')),


model1.add(Conv2D(64, kernel_size= (3,3), padding= 'valid', activation='relu'))
model1.add(BatchNormalization())
model1.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid')),

model1.add(Conv2D(128, kernel_size = (3,3), padding= 'valid', activation='relu'))
model1.add(BatchNormalization())
model1.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid')),

model1.add(Flatten())

model1.add(Dense(128, activation = 'relu'))
model1.add(Dropout(0.1))
model1.add(Dense(64, activation = 'relu'))
model1.add(Dropout(0.1))
model1.add(Dense(1, activation = 'sigmoid'))
#in o/p we layer we dont put dropout layer

"""Batch normalization is a technique used in the training neural network to make them faster and more stable. it actually make the trainig data mean as 0 and standard deviation as 1
same as normal distribution
"""

model1.summary()

model1.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics= ['accuracy'])

history1 = model1.fit(train_ds, epochs= 10, validation_data = validation_ds)

plt.plot(history1.history['accuracy'], color = 'red', label = 'train')
plt.plot(history1.history['val_accuracy'], color = 'blue', label = 'validation')
plt.legend()
plt.show()

plt.plot(history1.history['loss'], color = 'red', label = 'train')
plt.plot(history1.history['val_loss'], color = 'blue', label = 'validation')
plt.legend()
plt.show()

"""# Conclusion --- after normalization, somehow the model is not very much overfitted

# Object Detection
"""

import cv2

test_img = cv2.imread("/content/dog.jpg")

plt.imshow(test_img)
plt.show()

# shape

test_img.shape

# the image should be 256,256,3

"""reshape the image
resize is used to make changes in existing dimensions
reshape used to change the dimensions
"""

test_img = cv2.resize(test_img, (256,256))

test_img.shape

test_img = test_img.reshape(1,256,256,3)

test_img.shape

# making some prediction

result = model1.predict(test_img)

result

int(result)

